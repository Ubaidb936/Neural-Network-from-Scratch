{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91b05bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea88641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load two of the 10 classes (c1 is for Y=-1 and c2 is for Y=+1)\n",
    "def loaddigitdata(c1,c2,m):\n",
    "#     f = h5py.File('/usr/local/cs171/uspsall73.mat','r') \n",
    "    data = f.get('data') \n",
    "    data = np.array(data).astype(float)\n",
    "    X = np.concatenate((data[c1,:,:],data[c2,:,:]))\n",
    "    Y = np.concatenate((-np.ones((data.shape[1])),np.ones((data.shape[1]))))\n",
    "    \n",
    "    rs = np.random.RandomState(seed=132857) # setting seed so that dataset is consistent\n",
    "    p = rs.permutation(X.shape[0])\n",
    "    X = X[p] # this and next line make copies, but that's okay given how small our dataset is\n",
    "    Y = Y[p]\n",
    "    \n",
    "    trainX = X[0:m,:] # use the first m (after shuffling) for training\n",
    "    trainY = Y[0:m,np.newaxis]\n",
    "    validX = X[m:,:] # use the rest for validation\n",
    "    validY = Y[m:,np.newaxis]\n",
    "    return (trainX,trainY,validX,validY)\n",
    "\n",
    "\n",
    "def drawexample(x,ax=None): # takes an x *vector* and draws the image it encodes\n",
    "    if ax is None:\n",
    "        plt.imshow(np.reshape(x,(16,16)).T,cmap='gist_yarg')\n",
    "    else:\n",
    "        ax.imshow(np.reshape(x,(16,16)).T,cmap='gist_yarg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41712e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data, to differentiate between 7s and 9s\n",
    "# we will use on 1100 examples for training (50% of the data) and the other half for hold-out validation\n",
    "(trainX,trainY,validX,validY) = loaddigitdata(6,8,1100)\n",
    "means = trainX.mean(axis=0)\n",
    "stddevs = trainX.std(axis=0)\n",
    "stddevs[stddevs<1e-6] = 1.0\n",
    "trainX = (trainX-means)/stddevs # z-score normalization\n",
    "validX = (validX-means)/stddevs # apply same transformation to validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74c7ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert this cell to a code cell if you wish to see each of the examples, plotted\n",
    "f = plt.figure()\n",
    "f.set_size_inches(8,8)\n",
    "\n",
    "ax = f.add_subplot(111)\n",
    "plt.ion()\n",
    "f.canvas.draw()\n",
    "for exi in range(trainX.shape[0]):\n",
    "    display.clear_output(wait=True)\n",
    "    drawexample(trainX[exi,:])\n",
    "    digitid = (9 if trainY[exi]>0 else 7)\n",
    "    ax.set_title('y = '+str(int(trainY[exi]))+\" [\"+str(digitid)+\"]\")\n",
    "    display.display(f)\n",
    "    #time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6924d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def backwardProp(X, Y,  Wts, fx, lam):\n",
    "#     w1 = np.array([[0.80, 0.10, 0.30], [0.10, 0.00, 0.30], [0.00 ,-1.00, 0.50]])\n",
    "#     W2 = np.array([[-1.00, 0.50, -0.40, 1.10], [1.00, -0.2, -1.5, 0.20]])\n",
    "#     W3 = np.array([[-1, -3, 2]])\n",
    "    \n",
    "#     X = np.array([[2, -1], [1,3], [0, -2]])\n",
    "\n",
    "#     fx = nneval(X,Wts)\n",
    "\n",
    "   \n",
    "    DLF = -1 * (1 - (1/(1 + np.exp(-Y*fx))))*Y\n",
    "\n",
    "    W1 = Wts[0]\n",
    "    W2 = Wts[1]\n",
    "    W3 = Wts[2]\n",
    "\n",
    "    #####THINK ON THIS\n",
    "    delta3 = np.ones((len(X),1))\n",
    "\n",
    "    d2 = delta3.dot(W3)\n",
    "\n",
    "\n",
    "    delta2 = d2[:, 1:] * (1 - np.tanh(Z2)**2)\n",
    "\n",
    "\n",
    "    d1 = delta2.dot(W2)\n",
    "\n",
    "\n",
    "    delta1 = d1[:, 1:] * (1 - np.tanh(Z1)**2)\n",
    "\n",
    "\n",
    "    #Batch gradients\n",
    "    dw1 = np.zeros((len(W1), len(W1[0])))\n",
    "    dw2 = np.zeros((len(W2), len(W2[0])))\n",
    "    dw3 = np.zeros((len(W3), len(W3[0])))\n",
    " \n",
    "    for i in range(len(X)):\n",
    "        dw3 += DLF[i]*np.matmul(np.atleast_2d(delta3[i]).T,np.atleast_2d(A2[i]))\n",
    "        \n",
    "        \n",
    "        dw2 += DLF[i]*np.matmul(np.atleast_2d(delta2[i]).T,np.atleast_2d(A1[i]))\n",
    "\n",
    "        \n",
    "        dw1 += DLF[i]*np.matmul(np.atleast_2d(delta1[i]).T,np.atleast_2d(A0[i])) \n",
    "    \n",
    "    \n",
    "    dw3 += 2*lam*W3\n",
    "    dw2 += 2*lam*W2\n",
    "    dw1 += 2*lam*W1\n",
    "    \n",
    "    return [dw1,dw2,dw3] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def nneval(X,Wts):\n",
    "    # X is m-by-n (no \"extra 1s\" features added)\n",
    "    # Wts is a python array of 3 matrices\n",
    "    # Wts[0] is (n+1)-by-h (if there are h hidden units in the first layer)\n",
    "    # Wts[1] is (h+1)-by-q (if there are q hidden units in the second layer)\n",
    "    # Wts[2] is (q+1)-by-1\n",
    "    # This function  returns  matrix that is (m,1) -- the discriminant value for each example/row in X\n",
    "    \n",
    "    global A0, A1, A2\n",
    "    global Z1, Z2\n",
    "    \n",
    "    inputs = len(X[0])\n",
    "    \n",
    "    layer1HiddenUnitsCnt = len(Wts[0])\n",
    "    \n",
    "    layer2HiiddenUnitsCnt = len(Wts[1])\n",
    "    \n",
    "    \n",
    "    A0 = np.empty((0, inputs + 1))\n",
    "    \n",
    "    A1 = np.empty((0, layer1HiddenUnitsCnt + 1))\n",
    "    \n",
    "    A2 = np.empty((0, layer2HiiddenUnitsCnt + 1))\n",
    "    \n",
    "    \n",
    "    Z1 = np.empty((0, layer2HiiddenUnitsCnt))\n",
    "    \n",
    "    Z2 = np.empty((0, layer2HiiddenUnitsCnt))\n",
    "    \n",
    "    \n",
    "    X  = np.concatenate((np.ones((X.shape[0], 1)),X), axis = 1)\n",
    "    A0 = X\n",
    "    \n",
    "    \n",
    "    Z1 =  np.dot(A0, Wts[0].transpose())\n",
    "    A1 =  np.tanh(Z1)\n",
    "    A1  = np.concatenate((np.ones((A1.shape[0], 1)),A1), axis = 1)\n",
    "    \n",
    "    \n",
    "    Z2 =  np.dot(A1, Wts[1].transpose())\n",
    "    A2 =  np.tanh(Z2)\n",
    "    A2  = np.concatenate((np.ones((A2.shape[0], 1)),A2), axis = 1)\n",
    "    \n",
    "    Z3 =  np.dot(A2, Wts[2].transpose())\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    return Z3\n",
    "def trainneuralnet(X,Y,nhid,lam,printinfo=False):\n",
    "    \n",
    "    threshHold = 0.000001\n",
    "    # The number of examples (m) and number of input dimensions (n) of the input data\n",
    "    (m,n) = X.shape\n",
    "    \n",
    "    # This is the code that initializes the weight matrics:\n",
    "    W1 = (np.random.rand(nhid,n+1)*2-1)*np.sqrt(6.0/(n+nhid+1)) # weights to each hidden unit from the inputs (plus the added offset unit)\n",
    "    W2 = (np.random.rand(nhid//2,nhid+1)*2-1)*np.sqrt(6.0/(nhid+nhid//2+1))\n",
    "    W3 = (np.random.rand(1,nhid//2+1)*2-1)*np.sqrt(6.0/(nhid//2+2)) # weights to the single output unit from the hidden units (plus the offset unit)\n",
    "    W1[:,0] = 0 # good initializations for the constant terms\n",
    "    W2[:,0] = 0 #-nhid/2.0\n",
    "    W3[:,0] = 0 #-nhid/4.0\n",
    "    \n",
    "    Wts = [W1,W2,W3] \n",
    "    \n",
    "    Eg2=1 # a variable that is kept around for updating eta\n",
    "    \n",
    "    \n",
    "    ##1- Forward Propagation Results\n",
    "    fx = nneval(X,Wts)\n",
    "    \n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##2-- Calculate Total Loss\n",
    "    sig = 1/(1 + np.exp(-Y*fx))\n",
    "    \n",
    "    L1 = np.sum(-np.log(sig))\n",
    "    \n",
    "    ##2-a Regularization Part in the Loss\n",
    "    L2 = lam*(np.sum(Wts[0]*Wts[0]) + np.sum(Wts[1]*Wts[1]) + np.sum(Wts[2]*Wts[2]))\n",
    "    \n",
    "    \n",
    "    PreviousLoss = L1 + L2\n",
    "    \n",
    "    # print(PreviousLoss)\n",
    "    \n",
    "    # updates = backwardProp(X, Y,  Wts, fx, lam)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    while i <= 1700:\n",
    "        \n",
    "       \n",
    "     \n",
    "        \n",
    "        updates = backwardProp(X, Y,  Wts, fx, lam)\n",
    "        \n",
    "        sumofgrad2 = (np.sum(updates[0]*updates[0]) + np.sum(updates[1]*updates[1]) + np.sum(updates[2]*updates[2]))\n",
    "        \n",
    "        Eg2 = 0.9*Eg2 + 0.1*sumofgrad2\n",
    "        eta = 0.01/(np.sqrt((1e-10+Eg2)))\n",
    "        \n",
    "        \n",
    "        for j in range(3):\n",
    "            Wts[j] = Wts[j] - (eta * updates[j])\n",
    "            \n",
    "    \n",
    "        \n",
    "        fx_new = nneval(X,Wts)\n",
    "        \n",
    "        ##2-- Calculate Total Loss\n",
    "        sig = 1/(1 + np.exp(-Y*fx_new))\n",
    "    \n",
    "        L1 = np.sum(-np.log(sig))\n",
    "    \n",
    "        ##2-a Regularization Part in the Loss\n",
    "        L2 = lam*(np.sum(Wts[0]*Wts[0]) + np.sum(Wts[1]*Wts[1]) + np.sum(Wts[2]*Wts[2]))\n",
    "    \n",
    "        CurrentLoss = L1 + L2\n",
    "        \n",
    "        # print(PreviousLoss)\n",
    "        \n",
    "        ##Check the deffence b/w the current and previous Loss After 10 iteratins\n",
    "        if i > 0 and i % 10 == 0 and abs(CurrentLoss - PreviousLoss) < threshHold:\n",
    "            break\n",
    "        \n",
    "        PreviousLoss = CurrentLoss\n",
    "        \n",
    "        \n",
    "        \n",
    "        fx = fx_new\n",
    "        i +=1\n",
    "        \n",
    "\n",
    "    return Wts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4a85ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Wts = trainneuralnet(trainX,trainY,16,1e-5,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e15520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code added to do logistic regression, just for a comparison\n",
    "def learnlogreg_sklearn(X,Y,regstr=0.0,penalizeb=False):\n",
    "    X = np.hstack((np.ones((X.shape[0],1)),X))\n",
    "    Y = Y[:,0]\n",
    "    from sklearn.linear_model import LogisticRegression as lr\n",
    "    if regstr==0.0:\n",
    "        pen = 'none'\n",
    "        C = np.inf\n",
    "    else:\n",
    "        pen = 'l2'\n",
    "        C = 2.0/regstr\n",
    "\n",
    "    Y = (Y>0.5).astype(int)\n",
    "    if penalizeb:\n",
    "        lrres = lr(tol=1e-6,max_iter=100000,penalty=pen,C=C,\n",
    "                   fit_intercept=False,\n",
    "                   solver='newton-cg',multi_class='multinomial').fit(X,Y)\n",
    "        w = lrres.coef_[0,:]\n",
    "    else:\n",
    "        lrres = lr(tol=1e-6,max_iter=100000,penalty=pen,C=C,\n",
    "                   fit_intercept=True,\n",
    "                   solver='newton-cg',multi_class='multinomial').fit(X[:,1:],Y)\n",
    "        w = np.hstack((lrres.intercept_,lrres.coef_[0,:]))\n",
    "    return w\n",
    "\n",
    "def predictlogreg(X,w):\n",
    "    return np.hstack((np.ones((X.shape[0],1)),X))@w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47199f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to update the plot as new results are generated\n",
    "def setupfig():\n",
    "    f = plt.figure()\n",
    "    f.set_size_inches(8,8)\n",
    "    ax = f.add_subplot(111)\n",
    "    plt.ion()\n",
    "    f.canvas.draw()\n",
    "    return (f,ax)\n",
    "\n",
    "def plotit(lams,nhiddens,erates,f,ax):\n",
    "    ax.clear()\n",
    "    for i in range(nhiddens.shape[0]):\n",
    "        ax.plot(lams,erates[:,i],'*-')\n",
    "    ax.set_yscale('log',subs=[1,2,3,4,5,6,7,8,9])\n",
    "    ax.set_yticks([0.1,0.01])\n",
    "    ax.set_xscale('log')\n",
    "    f.canvas.draw()\n",
    "    ax.set_xlabel('lambda')\n",
    "    ax.set_ylabel('validation error rate')\n",
    "    ax.legend([(('# hidden units = '+str(x)+','+str(x//2)) if x>0 else 'logistic regression') for x in nhiddens])\n",
    "    display.display(f)\n",
    "    display.clear_output(wait=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ff2e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def errorrate(Y,predy):\n",
    "    predy = np.sign(predy)\n",
    "    return (predy!=Y).mean()\n",
    "    \n",
    "def multirestart(trainX,trainY,nhid,lam,ntries):\n",
    "    besterrsc = 2.0\n",
    "    for i in range(ntries):\n",
    "        Wts = trainneuralnet(trainX,trainY,nhid,lam)\n",
    "        errsc = errorrate(trainY,nneval(trainX,Wts))\n",
    "        if errsc<besterrsc:\n",
    "            returnWts = Wts\n",
    "            besterrsc = errsc\n",
    "    return returnWts\n",
    "\n",
    "nhiddens = np.array([0,2,8,16])\n",
    "lams = np.logspace(-12,3,10)\n",
    "erates = np.empty([lams.shape[0],nhiddens.shape[0]])\n",
    "erates[:,:] = np.nan\n",
    "\n",
    "(f,ax) = setupfig()\n",
    "    \n",
    "for ni, nhid in enumerate(nhiddens):\n",
    "    for li, lam in reversed(list(enumerate(lams))):\n",
    "        if nhid==0:\n",
    "            w = learnlogreg_sklearn(trainX,trainY,lam)\n",
    "            predy = predictlogreg(validX,w)[:,np.newaxis]\n",
    "        else:\n",
    "            Wts = multirestart(trainX,trainY,nhid,lam,2) #trainneuralnet(trainX,trainY,nhid,lam)\n",
    "            predy = nneval(validX,Wts)\n",
    "        erates[li,ni] = errorrate(validY,predy)\n",
    "        \n",
    "        plotit(lams,nhiddens,erates,f,ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfccb86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
